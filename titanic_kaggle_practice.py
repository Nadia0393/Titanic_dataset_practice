# -*- coding: utf-8 -*-
"""Titanic_kaggle_practice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v4a3loRrfgI3WeaP3YA3Hn5tJGb4mWh2

Predicting the survival of Titanic Passengers
"""

#importing the libraries
import numpy as np # linear algebra
import pandas as pd #data processing
import seaborn as sns # Data Visualisation

#Matplotlib
from matplotlib import pyplot as plt
from matplotlib import style

# Algorithms
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB

"""Getting the data"""

train_df=pd.read_csv('/content/train.csv')
test_df=pd.read_csv('/content/test.csv')

train_df.head(10)

"""We need to take care of NaN values . Also we need to convert the features to the numeric so that the machine learning algorithm process it .

"""

train_df.info()

train_df.describe()

"""From the above statistics , we could see that 38% of passengers have survived the disaster .Also the passenger ages range from 0.4 to 80 .25% of passengers are of age 20 ,50% of passengers are of age 28,75% of passengers are of age 75 and max age of passengers are 80"""

train_df.head(10)

"""Lets check what all the data are having missing data .Lets dig little more .

"""

total = train_df.isnull().sum().sort_values(ascending=False)
total.head()

"""Since Cabin got 687 null values , we will drop the column .Embarked has only got 2 null values .Age got 177 missing value and we will be able to deal with it ."""

train_df.columns.values

"""Totally 11 features and 1 target variable .i am asking myself and see what features contribute to the passengers high survival rate ?
passengerId, Ticket and Name doesnt add more sense to the survival rate . Lets check the other features .

Lets plot the histogram and see how the graph hints us with the visualisation .
"""

survived = 'survived'
not_survived = 'not survived'
fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))
Female=train_df[train_df['Sex']=='female']
Male=train_df[train_df['Sex']=='male']
ax = sns.distplot(Female[Female['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)
ax = sns.distplot(Female[Female['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)
ax.legend()
ax.set_title('Female')
ax = sns.distplot(Male[Male['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)
ax = sns.distplot(Male[Male['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)
ax.legend()
ax.set_title('Male')

"""Embarked, PClass , Sex"""

FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)
FacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )
FacetGrid.add_legend()

sns.barplot(x='Pclass', y='Survived', data=train_df)

"""If the person is in Pclass=1 , the sruvival rate is higher than the other 2 Pclasses -> 2 & 3"""

grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)
grid.map(plt.hist, 'Age', alpha=.5, bins=20)
grid.add_legend()

"""From the above histogram ,we can see that the people in the Pclass = 1 sruvival rate is higher than Pclass=2 & 3 .Also we can see that probability that people will not survive if they are in the Pclass=3

Lets combine SibSp and ParCh fields to see who are not alone and to get their counts
"""

data=[train_df,test_df]

for dataset in data:
    dataset['relatives']=dataset['SibSp'] + dataset['Parch']
    dataset.loc[dataset['relatives']>0,'not alone']=1
    dataset.loc[dataset['relatives']==0,'not alone']=0
    dataset['not alone']=dataset['not alone'].astype(int)
train_df['not alone'].value_counts()

axes = sns.factorplot('relatives','Survived', 
                      data=train_df, aspect = 2.5, )

"""Having RElatives between 1 and 3 survived better than having relatives less than 1 and higher than 3 .

Lets go over the Data Preprocessing steps ⁉
1.   Dropping the Passenger ID columns as its not contributing to the survival rate .
2.   handling the missing data values . 
3. Changing the Cabin column to Deck with the Code extracted from the cabin column .
4.To tackle the missing age value , we can create the array filled with random numbers based on the mean age value .
"""

train_df=train_df.drop(['PassengerId'],axis=1)

train_df.head(10)

"""Lets deal with the missing data . Cabin (687), Embarked (2) and Age (177) ."""

import re
deck = {"A": 1, "B": 2, "C": 3, "D": 4, "E": 5, "F": 6, "G": 7, "U": 8}
data = [train_df, test_df]
for dataset in data:
    dataset['Cabin'] = dataset['Cabin'].fillna("U0")
    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile("([a-zA-Z]+)").search(x).group())
    dataset['Deck'] = dataset['Deck'].map(deck)
    dataset['Deck'] = dataset['Deck'].fillna(0)
    dataset['Deck'] = dataset['Deck'].astype(int)
#Dropping the Cabin column
train_df = train_df.drop(['Cabin'], axis=1)
test_df = test_df.drop(['Cabin'], axis=1)

train_df.head(10)

"""Handling the missing Age value """

data = [train_df, test_df]


for dataset in data:
  mean=dataset['Age'].mean() # calculating the mean 
  std=dataset['Age'].mean() # Calculating the standard deviation
  is_null=dataset['Age'].isnull().sum() 
  rand_age=np.random.randint(mean-std,mean+std,size=is_null)
  age_copy=dataset['Age'].copy()
  age_copy[np.isnan(age_copy)]=rand_age
  dataset['Age']=age_copy
  dataset["Age"] = train_df["Age"].astype(int)

dataset['Age'].isnull().sum()

train_df.head(10)

"""Lets see the Embarked column for the missing values .The most frequent appeared value is "S" . Lets fill the missing values with "S" as there is only 2 missing values ."""

train_df['Embarked'].describe()

common_value = 'S'
data = [train_df, test_df]

for dataset in data:
    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)

dataset['Embarked'].isnull().sum()

"""Lets convert the features :  Fare - Float value 
Ticket , 
"""

data = [train_df, test_df]

for dataset in data:
    dataset['Fare'] = dataset['Fare'].fillna(0)
    dataset['Fare'] = dataset['Fare'].astype(int)

train_df.info()

"""Now we will work on the Name feature ."""

train_df['Name'].describe()

data = [train_df, test_df]
titles = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5}

for dataset in data:
    # extract titles
    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)
    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\
                                            'Major', 'Rev', 'Sir','Jonkheer', 'Dona'], 'Rare')
    dataset['Title'] = dataset['Title'].replace(['Mlle','Ms'], 'Miss')
    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')
    dataset['Title'] = dataset['Title'].map(titles)# Converting titles to numbers
    dataset['Title'] = dataset['Title'].fillna(0) # Filling NaN with 0


train_df = train_df.drop(['Name'], axis=1)
test_df = test_df.drop(['Name'], axis=1)

train_df.head(10)

"""Converting ‘Sex’ feature into numeric."""

data = [train_df, test_df]
gender = {"male": 0, "female": 1}

for dataset in data:
  dataset['Sex']=dataset['Sex'].map(gender)

train_df.head()

"""Dropping the ticket column as its almost having unique values /Converting them will not add more meaning to the current data"""

train_df['Ticket'].describe()

train_df = train_df.drop(['Ticket'], axis=1)
test_df = test_df.drop(['Ticket'], axis=1)

"""Embarked : Converting Embarked to Numeric feature"""

dataset['Embarked'].describe()

ports = {"S": 0, "C": 1, "Q": 2}
data = [train_df, test_df]

for dataset in data:
    dataset['Embarked'] = dataset['Embarked'].map(ports)

dataset.info()

"""Create Categories : We will now create categories withing the existing features 

"""

data = [train_df, test_df]
for dataset in data:
    #dataset['Age'] = dataset['Age'].astype(int)
    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0
    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1
    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2
    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3
    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4
    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5
    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6
    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6
train_df['Age'].value_counts()

"""Lets dig into the 'Fare' feature

"""

train_df.head(10)

dataset['Fare'].describe()

data = [train_df, test_df]

for dataset in data:
    dataset.loc[ dataset['Fare'] <= 7, 'Fare'] = 0
    dataset.loc[(dataset['Fare'] > 7) & (dataset['Fare'] <= 14), 'Fare'] = 1
    dataset.loc[(dataset['Fare'] > 14) & (dataset['Fare'] <= 31), 'Fare']   = 2
    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3
    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4
    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5

"""I have analysed the existing features and i am gonna create new out of the existing data"""

train_df.head()

for dataset in data:
    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)
    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)
# Let's take a last look at the training set, before we start training the models.
train_df.head(10)

"""Lets create some machine learning models 
X_train,Y_train
X_test,Y_test
X_train ->without the target value .. so droppping the survived column
"""

X_train=train_df.drop("Survived",axis=1)
Y_train=train_df["Survived"]

X_test  = test_df.drop("PassengerId", axis=1).copy()

"""Stochastic Gradient Descent"""

sgd = linear_model.SGDClassifier(max_iter=5, tol=None)
sgd.fit(X_train, Y_train)
y_pred=sgd.predict(X_test)
sgd.score(X_train, Y_train)
acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)

"""Random Forest"""

random_forest = RandomForestClassifier(n_estimators=100)
random_forest.fit(X_train, Y_train)
Y_prediction = random_forest.predict(X_test)
random_forest.score(X_train, Y_train)
acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)

results = pd.DataFrame({
    'Model': [
              'Random Forest', 
              'Stochastic Gradient Decent'],
    'Score': [
              acc_random_forest, 
              acc_sgd]})
result_df = results.sort_values(by='Score', ascending=False)
result_df = result_df.set_index('Score')
result_df.head(9)

from sklearn.model_selection import cross_val_score
rf = RandomForestClassifier(n_estimators=100)
scores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = "accuracy")
print("Scores:", scores)
print("Mean:", scores.mean())
print("Standard Deviation:", scores.std())